{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified .\\notMNIST_large.tar.gz\n",
      "Found and verified .\\notMNIST_small.tar.gz\n",
      ".\\notMNIST_large already present - Skipping extraction of .\\notMNIST_large.tar.gz.\n",
      "['.\\\\notMNIST_large\\\\A', '.\\\\notMNIST_large\\\\B', '.\\\\notMNIST_large\\\\C', '.\\\\notMNIST_large\\\\D', '.\\\\notMNIST_large\\\\E', '.\\\\notMNIST_large\\\\F', '.\\\\notMNIST_large\\\\G', '.\\\\notMNIST_large\\\\H', '.\\\\notMNIST_large\\\\I', '.\\\\notMNIST_large\\\\J']\n",
      ".\\notMNIST_small already present - Skipping extraction of .\\notMNIST_small.tar.gz.\n",
      "['.\\\\notMNIST_small\\\\A', '.\\\\notMNIST_small\\\\B', '.\\\\notMNIST_small\\\\C', '.\\\\notMNIST_small\\\\D', '.\\\\notMNIST_small\\\\E', '.\\\\notMNIST_small\\\\F', '.\\\\notMNIST_small\\\\G', '.\\\\notMNIST_small\\\\H', '.\\\\notMNIST_small\\\\I', '.\\\\notMNIST_small\\\\J']\n",
      ".\\notMNIST_large\\A.pickle already present - Skipping pickling.\n",
      ".\\notMNIST_large\\B.pickle already present - Skipping pickling.\n",
      "Pickling .\\notMNIST_large\\C.pickle.\n",
      ".\\notMNIST_large\\C\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.142258\n",
      "Standard deviation: 0.439806\n",
      "Pickling .\\notMNIST_large\\D.pickle.\n",
      ".\\notMNIST_large\\D\n",
      "Could not read: .\\notMNIST_large\\D\\VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file '.\\\\notMNIST_large\\\\D\\\\VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.0573678\n",
      "Standard deviation: 0.455648\n",
      "Pickling .\\notMNIST_large\\E.pickle.\n",
      ".\\notMNIST_large\\E\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.069899\n",
      "Standard deviation: 0.452942\n",
      "Pickling .\\notMNIST_large\\F.pickle.\n",
      ".\\notMNIST_large\\F\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.125583\n",
      "Standard deviation: 0.44709\n",
      "Pickling .\\notMNIST_large\\G.pickle.\n",
      ".\\notMNIST_large\\G\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0945814\n",
      "Standard deviation: 0.44624\n",
      "Pickling .\\notMNIST_large\\H.pickle.\n",
      ".\\notMNIST_large\\H\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0685221\n",
      "Standard deviation: 0.454232\n",
      "Pickling .\\notMNIST_large\\I.pickle.\n",
      ".\\notMNIST_large\\I\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: 0.0307862\n",
      "Standard deviation: 0.468899\n",
      "Pickling .\\notMNIST_large\\J.pickle.\n",
      ".\\notMNIST_large\\J\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.153358\n",
      "Standard deviation: 0.443656\n",
      "Pickling .\\notMNIST_small\\A.pickle.\n",
      ".\\notMNIST_small\\A\n",
      "Could not read: .\\notMNIST_small\\A\\RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file '.\\\\notMNIST_small\\\\A\\\\RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.132626\n",
      "Standard deviation: 0.445128\n",
      "Pickling .\\notMNIST_small\\B.pickle.\n",
      ".\\notMNIST_small\\B\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: 0.00535609\n",
      "Standard deviation: 0.457115\n",
      "Pickling .\\notMNIST_small\\C.pickle.\n",
      ".\\notMNIST_small\\C\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.141521\n",
      "Standard deviation: 0.44269\n",
      "Pickling .\\notMNIST_small\\D.pickle.\n",
      ".\\notMNIST_small\\D\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0492167\n",
      "Standard deviation: 0.459759\n",
      "Pickling .\\notMNIST_small\\E.pickle.\n",
      ".\\notMNIST_small\\E\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0599148\n",
      "Standard deviation: 0.45735\n",
      "Pickling .\\notMNIST_small\\F.pickle.\n",
      ".\\notMNIST_small\\F\n",
      "Could not read: .\\notMNIST_small\\F\\Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file '.\\\\notMNIST_small\\\\F\\\\Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.118185\n",
      "Standard deviation: 0.452279\n",
      "Pickling .\\notMNIST_small\\G.pickle.\n",
      ".\\notMNIST_small\\G\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0925503\n",
      "Standard deviation: 0.449006\n",
      "Pickling .\\notMNIST_small\\H.pickle.\n",
      ".\\notMNIST_small\\H\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0586893\n",
      "Standard deviation: 0.458759\n",
      "Pickling .\\notMNIST_small\\I.pickle.\n",
      ".\\notMNIST_small\\I\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: 0.0526451\n",
      "Standard deviation: 0.471894\n",
      "Pickling .\\notMNIST_small\\J.pickle.\n",
      ".\\notMNIST_small\\J\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.151689\n",
      "Standard deviation: 0.448014\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "\n",
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n",
    "\n",
    "\n",
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d112d776d8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d112ceb518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "a_list = pickle.load(open(\"notMNIST_small/A.pickle\",\"rb\"))\n",
    "random_letter = random.choice(a_list)\n",
    "%matplotlib inline\n",
    "plt.imshow(random_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A training data count: 52909\n",
      "A testing data count: 1872\n",
      "B training data count: 52911\n",
      "B testing data count: 1873\n",
      "C training data count: 52912\n",
      "C testing data count: 1873\n",
      "D training data count: 52911\n",
      "D testing data count: 1873\n",
      "E training data count: 52912\n",
      "E testing data count: 1873\n",
      "F training data count: 52912\n",
      "F testing data count: 1872\n",
      "G training data count: 52912\n",
      "G testing data count: 1872\n",
      "H training data count: 52912\n",
      "H testing data count: 1872\n",
      "I training data count: 52912\n",
      "I testing data count: 1872\n",
      "J training data count: 52911\n",
      "J testing data count: 1872\n"
     ]
    }
   ],
   "source": [
    "letters = [chr(ord('A') + i) for i in range(0,10)]\n",
    "for letter in letters:\n",
    "    letter_train_data = pickle.load(open('notMNIST_large/'+ letter + '.pickle','rb'))\n",
    "    print(letter + \" training data count: \"+str(len(letter_train_data)))\n",
    "    letter_test_data = pickle.load(open('notMNIST_small/'+ letter + '.pickle','rb'))\n",
    "    print(letter + \" testing data count: \"+str(len(letter_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "try:\n",
    "    f=open(pickle_file, 'wb')\n",
    "    save={\n",
    "        'train_dataset':train_dataset,\n",
    "        'train_labels':train_labels,\n",
    "        'valid_dataset':valid_dataset,\n",
    "        'valid_labels':valid_labels,\n",
    "        'test_dataset':test_dataset,\n",
    "        'test_labels':test_labels,\n",
    "    }\n",
    "    pickle.dump(save,f,pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to ',pickle_file, ':',e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size:  690800512\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size: ',statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1301\n",
      "3465\n",
      "193\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "all_data = pickle.load(open('notMNIST.pickle','rb'))\n",
    "\n",
    "def count_dup(dataset1, dataset2):\n",
    "    hashes = [hashlib.sha1(x).hexdigest() for x in dataset1]\n",
    "    dup_indices = []\n",
    "    for i in dataset2:\n",
    "        if hashlib.sha1(i).hexdigest() in hashes:\n",
    "            dup_indices.append(i)\n",
    "    return len(dup_indices)\n",
    "print(count_dup(all_data['train_dataset'],all_data['test_dataset']))\n",
    "print(count_dup(all_data['valid_dataset'],all_data['train_dataset']))\n",
    "print(count_dup(all_data['valid_dataset'],all_data['test_dataset']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = all_data['train_dataset']\n",
    "train_labels = all_data['train_labels']\n",
    "test_dataset = all_data['test_dataset']\n",
    "test_labels = all_data['test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 trainsamples score: 0.7082\n",
      "1000 trainsamples score: 0.8292\n",
      "5000 trainsamples score: 0.8462\n",
      "10000 trainsamples score: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def get_score(train_dataset,train_labels, test_dataset, test_labels):\n",
    "    model  = LogisticRegression()\n",
    "    train_flat_dataset = np.array([x.flatten() for x in train_dataset])\n",
    "    test_flat_dataset = np.array([x.flatten() for x in test_dataset])\n",
    "    model.fit(train_flat_dataset, train_labels)\n",
    "    \n",
    "    return model.score(test_flat_dataset, test_labels)\n",
    "\n",
    "print(\"50 Sample score: \",str(get_score(train_dataset[:50],train_labels[:50],test_dataset[:50],test_labels[:50])))\n",
    "print(\"100 Sample score: \",str(get_score(train_dataset[:100],train_labels[:100],test_dataset[:100],test_labels[:100])))\n",
    "print(\"1000 Sample score: \",str(get_score(train_dataset[:1000],train_labels[:1000],test_dataset[:1000],test_labels[:1000])))\n",
    "print(\"5000 Sample score: \",str(get_score(train_dataset[:5000],train_labels[:5000],test_dataset[:5000],test_labels[:5000])))\n",
    "print(\"Sample score: \",str(get_score(train_dataset,train_labels,test_dataset,test_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
